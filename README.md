# Code Repository for Mixture of Experts Filter (MoE-F) 

Filtered not Mixed: Filtering-Based Online Gating for Mixture of Large Language Models

The following is a conceptual flow showing how MoE-F works: 
![Mixture of Experts](./imgs/mixture_of_experts_v3_8fps.gif)


Examining a cross-sectional time-window snapshots allows a better understanding. 

![Market Movement Plot](./imgs/market_movement_plot.png)



## Expert Weights Heatmap <a name="expert-weights-heatmap">

<!--
[Expert Weights Heatmap](./imgs/expert_weights_heatmap_coolwarm.pdf)
->
![Expert Weights Heatmap](./imgs/expert_weights_heatmap_coolwarm.png)


